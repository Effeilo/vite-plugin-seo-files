**EN** | [FR](./fr/README.md)

<div>
  <img src="https://browserux.com/assets/img/logo/logo-vite-plugin-seo-pages.png" alt="logo vite-plugin-seo-pages"/>
</div>

# vite-plugin-seo-files

**`vite-plugin-seo-files`** is a Vite plugin that automates the generation of essential technical SEO files (`sitemap.xml`, `robots.txt`) **after the build**. It supports both ESM and CommonJS through dual exports.

It integrates easily into your Vite configuration without any `.env` dependency and works with all frameworks (static HTML, React, Vue, Svelte‚Ä¶).

> A practical way to improve indexing and crawling for your static sites or SPAs.

[![npm version](https://img.shields.io/npm/v/vite-plugin-seo-files.svg)](https://www.npmjs.com/package/vite-plugin-seo-files)
![vite compatibility](https://img.shields.io/badge/Vite-4%2B%20%7C%205%2B-646CFF.svg?logo=vite&logoColor=white)


## üöÄ Installation

```bash
npm install vite-plugin-seo-files --save-dev
```

## ‚öôÔ∏è Usage

Add the plugin in your `vite.config.js` or `vite.config.mjs` file:

```js
import { defineConfig } from 'vite';
import seoFiles from 'vite-plugin-seo-files';

export default defineConfig({
    plugins: [
        seoFiles({
            siteUrl: 'https://example.com', // ‚úÖ Required
            generateSitemap: true, // optional, default: true
            generateRobots: true, // optional, default: true
            exclude: ['test.html', 'drafts/**'], // optional, glob patterns
            disallow: ['/private/', '/secret.html'] // optional, robots.txt disallow rules
        })
    ]
});
```

## üßæ Generated Files

After `vite build`, the plugin automatically writes to `dist/`:

| File          | Description                                        |
|---------------|----------------------------------------------------|
| `sitemap.xml` | List of HTML pages with `lastmod`, `priority`     |
| `robots.txt`  | Access rules for search engine crawlers            |

## üîß Available Options

| Option            | Type       | Default   | Description                                                      |
|-------------------|------------|-----------|------------------------------------------------------------------|
| `siteUrl`         | `string`   | -         | Base site URL used in `sitemap.xml` and `robots.txt`             |
| `generateSitemap` | `boolean`  | `true`    | Enable/disable sitemap generation                                |
| `generateRobots`  | `boolean`  | `true`    | Enable/disable robots.txt generation                             |
| `exclude`         | `string[]` | `[]`      | List of glob patterns to exclude from the sitemap                |
| `disallow`        | `string[]` | `[]`      | List of paths to disallow in robots.txt                          |

## ‚ú® Included Features

- Extracts real `lastmod` modification dates from `.html` files
- Compatible with all Vite projects (SPA, MPA, static)
- Custom exclusion via `exclude`
- robots.txt disallow rules via `disallow`
- No need for `.env` configuration

## üìÅ Examples

### Structure

```bash
dist/
‚îú‚îÄ‚îÄ index.html
‚îú‚îÄ‚îÄ about/
‚îú   ‚îî‚îÄ‚îÄ index.html
‚îú‚îÄ‚îÄ offline.html
‚îú‚îÄ‚îÄ sitemap.xml # generated automatically
‚îî‚îÄ‚îÄ robots.txt # generated automatically
```

### sitemap.xml

```xml
<?xml version="1.0" encoding="UTF-8"?>
<urlset xmlns="http://www.sitemaps.org/schemas/sitemap/0.9">
    <url>
        <loc>https://browserux.com/offline.html</loc>
        <lastmod>2025-06-07</lastmod>
        <priority>0.50</priority>
    </url>
    <url>
        <loc>https://browserux.com/</loc>
        <lastmod>2025-06-07</lastmod>
        <priority>1.00</priority>
    </url>
    <url>
        <loc>https://browserux.com/about/</loc>
        <lastmod>2025-06-07</lastmod>
        <priority>0.50</priority>
    </url>
</urlset>
```

### robots.txt

```bash
# https://www.robotstxt.org/

# Allow all crawlers full access
User-agent: *

# Prevent indexing of sensitive or non-public areas
Disallow: /private/
Disallow: /secret.html

# Sitemap file
Sitemap: https://browserux.com/sitemap.xml
```

## üìå Best Practices

- The `siteUrl` field must be an absolute URL (starting with `https://`)
- The plugin only runs during `vite build`, not during development
- Use `exclude` to ignore draft or error pages from the sitemap
- Use `disallow` to prevent indexing of specific paths in robots.txt

## ‚öñÔ∏è License

MIT ¬© 2025 [Effeilo](https://github.com/Effeilo)